{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " loading the my one file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import pickle\n",
    "\n",
    "# Load the .pkl file, specifying the encoding\n",
    "with open('D:\\WESAD\\S3\\S3.pkl', 'rb') as file:\n",
    "    data_dict = pickle.load(file, encoding='latin1')  # Try 'latin1' or 'utf-8'\n",
    "\n",
    "# Display the data\n",
    "print(data_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " just for calculating reading and combining the data, how it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import numpy as np\n",
    "ecg_signal = data_dict['signal']['chest']['ECG'].flatten()  # Flatten to make it 1D array\n",
    "label = data_dict['label']\n",
    "\n",
    "# Combine ECG signal and label into a structured array\n",
    "combined_data = np.zeros(len(ecg_signal), dtype=[('ECG', np.float64), ('label', np.int32)])\n",
    "combined_data['ECG'] = ecg_signal\n",
    "combined_data['label'] = label\n",
    "\n",
    "# Print combined data\n",
    "print(\"Combined ECG Signal and Label Data:\")\n",
    "print(combined_data)\n",
    "print(\"Shape of combined data:\", combined_data.shape)\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Combine ECG signal and label into a 2D structured array\n",
    "combined_data = np.empty((len(ecg_signal), ), dtype=[('ECG', np.float64), ('label', np.int32)])\n",
    "combined_data['ECG'] = ecg_signal\n",
    "combined_data['label'] = label\n",
    "\n",
    "# Extract the rows where the label is 2\n",
    "label_2_data = combined_data[combined_data['label'] == 2]\n",
    "label_3_data = combined_data[combined_data['label'] == 3]\n",
    "label_1_data = combined_data[combined_data['label'] == 1]\n",
    "\n",
    "# Print the filtered data\n",
    "print(\"Data corresponding to label 1:\")\n",
    "print(label_1_data)\n",
    "size1=len(label_1_data)\n",
    "print(\"size with label1\",size1)\n",
    "\n",
    "\n",
    "'''print(\"Data corresponding to label 2:\")\n",
    "print(label_2_data)'''\n",
    "size2=len(label_2_data)\n",
    "print(\"size with label2\",size2)\n",
    "\n",
    "#print(\"Data corresponding to label 3:\")\n",
    "size3=len(label_3_data)\n",
    "print(\"size with label3\",size3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " calculating the epochs into user input time segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Define the parameters\n",
    "\n",
    "sampling_frequency = 700  # in Hz\n",
    "\n",
    "# Function to calculate the number of segments\n",
    "def calculate_segments(sample_size, segment_time):\n",
    "    segment_samples = segment_time * sampling_frequency\n",
    "    number_of_segments = sample_size / segment_samples\n",
    "    return number_of_segments\n",
    "\n",
    "# Get user input for the segment time\n",
    "segment_time = float(input(\"Enter the segment time in seconds: \"))\n",
    "\n",
    "# Get user input for the three sample sizes\n",
    "sample_size1 = size1\n",
    "sample_size2 = size2\n",
    "sample_size3 = size3\n",
    "\n",
    "# Calculate the number of segments for each sample size\n",
    "number_of_segments1 = calculate_segments(sample_size1, segment_time)\n",
    "number_of_segments2 = calculate_segments(sample_size2, segment_time)\n",
    "number_of_segments3 = calculate_segments(sample_size3, segment_time)\n",
    "\n",
    "# Print the results\n",
    "print(f\"duration of each segment: {size1 / 700:.2f} seconds\")\n",
    "print(f\"Number of segments for sample size {sample_size1}: {number_of_segments1}\")\n",
    "\n",
    "print(f\"duration of each segment: {size2 / 700:.2f} seconds\")\n",
    "print(f\"Number of segments for sample size {sample_size2}: {number_of_segments2}\")\n",
    "\n",
    "print(f\"duration of each segment: {size3 / 700:.2f} seconds\")\n",
    "print(f\"Number of segments for sample size {sample_size3}: {number_of_segments3}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " collecting baseline,stress and amuse data from all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Define the directory path\n",
    "directory_path = 'D:\\\\WESAD\\\\wesad4'\n",
    "\n",
    "# Function to load data from a pickle file\n",
    "def load_pickle_file(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        data = pickle.load(f, encoding='latin1')\n",
    "    return data\n",
    "\n",
    "# Get a list of all pickle files in the directory\n",
    "all_files = [f for f in os.listdir(directory_path) if f.endswith('.pkl')]\n",
    "\n",
    "\n",
    "\n",
    "# Initialize a list to hold the loaded data\n",
    "loaded_data1 = []\n",
    "loaded_data2 = []\n",
    "loaded_data3 = []\n",
    "\n",
    "# Iterate through the selected pickle files and load them\n",
    "for file_name in all_files:\n",
    "    file_path = os.path.join(directory_path, file_name)\n",
    "    data = load_pickle_file(file_path)\n",
    "    \n",
    "    ecg_signal = data['signal']['chest']['ECG'].flatten()  # Flatten to make it 1D array\n",
    "    label = data['label']\n",
    "\n",
    "# Combine ECG signal and label into a structured array\n",
    "    combined_data = np.zeros(len(ecg_signal),dtype=[('ECG', np.float64), ('label', np.int32)])\n",
    "    combined_data['ECG'] = ecg_signal\n",
    "    combined_data['label'] = label\n",
    "\n",
    "# Print combined data\n",
    "\n",
    "    '''print(\"Combined ECG Signal and Label Data:\")\n",
    "    print(combined_data)\n",
    "    print(\"Shape of combined data:\", combined_data.shape)\n",
    "    '''\n",
    "# Extract the rows where the label is 2\n",
    "    label_1_data = combined_data[combined_data['label'] == 1]\n",
    "    label_2_data = combined_data[combined_data['label'] == 2]\n",
    "    label_3_data = combined_data[combined_data['label'] == 3]\n",
    "    '''print(label_1_data)\n",
    "    print(label_2_data)\n",
    "    print(label_3_data)'''\n",
    "    loaded_data1.append(label_1_data)\n",
    "    loaded_data2.append(label_2_data)\n",
    "    loaded_data3.append(label_3_data)\n",
    "'''print(loaded_data1)\n",
    "print(loaded_data2)\n",
    "print(loaded_data3)'''\n",
    "\n",
    "# Extract ECG values\n",
    "ecg_values1 = [arr['ECG'] for arr in loaded_data1]\n",
    "ecg_values2 = [arr['ECG'] for arr in loaded_data2]\n",
    "ecg_values3 = [arr['ECG'] for arr in loaded_data3]\n",
    "\n",
    "# ecg_values now contains a list of ECG values from each array\n",
    "\n",
    "print(len(ecg_values1))\n",
    "print(len(ecg_values2))\n",
    "print(len(ecg_values3))\n",
    "\n",
    "baseline_arrayy = np.concatenate(ecg_values1)\n",
    "stress_arrayy = np.concatenate(ecg_values2)\n",
    "amuse_arrayy = np.concatenate(ecg_values3)\n",
    "\n",
    "print(baseline_arrayy)\n",
    "print(stress_arrayy)\n",
    "print(amuse_arrayy)\n",
    "print(\"length of baseline samples \",len(baseline_arrayy))\n",
    "print(\"length of stress sample \",len(stress_arrayy))\n",
    "print(\"length of amuse sample\",len(amuse_arrayy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import numpy as np\n",
    "import scipy.signal as signal\n",
    "import matplotlib.pyplot as plt\n",
    "def preprocess_ecg(ecg_signal, fs):\n",
    "    # Bandpass filter to remove noise\n",
    "    lowcut = 0.5  # 0.5 Hz\n",
    "    highcut = 45.0  # 45 Hz\n",
    "    nyquist = 0.5 * fs\n",
    "    low = lowcut / nyquist\n",
    "    high = highcut / nyquist\n",
    "    b, a = signal.butter(1, [low, high], btype='band')\n",
    "    filtered_ecg = signal.filtfilt(b, a, ecg_signal)\n",
    "    \n",
    "    normalized_ecg = (filtered_ecg - np.mean(filtered_ecg)) / np.std(filtered_ecg)\n",
    "    return normalized_ecg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "fs = 700 # Sampling frequency (Hz)\n",
    "ecg_signal = baseline_arrayy # Load your ECG data\n",
    "baseline_array = preprocess_ecg(ecg_signal, fs)\n",
    "\n",
    "time = np.arange(0, len(ecg_signal)) / fs\n",
    "plt.figure(figsize=(12,10)) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the zoom range in seconds\n",
    "zoom_start = 0\n",
    "zoom_end = 10  # Zoom into the first 10 seconds\n",
    "\n",
    "# Plot original ECG signal\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(time, ecg_signal, label='Original ECG Signal')\n",
    "plt.xlim(zoom_start, zoom_end)\n",
    "plt.title('Original Baseline ECG Signal (Zoomed In)')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend()\n",
    "\n",
    "# Plot normalized ECG signal\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(time,baseline_array, label='Normalized ECG Signal', color='orange')\n",
    "plt.xlim(zoom_start, zoom_end)\n",
    "plt.title('Normalized Baseline ECG Signal')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude (normalized)')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import numpy as np\n",
    "import scipy.signal as signal\n",
    "def preprocess_ecg(ecg_signal, fs):\n",
    "    # Bandpass filter to remove noise\n",
    "    lowcut = 0.5  # 0.5 Hz\n",
    "    highcut = 45.0  # 45 Hz\n",
    "    nyquist = 0.5 * fs\n",
    "    low = lowcut / nyquist\n",
    "    high = highcut / nyquist\n",
    "    b, a = signal.butter(1, [low, high], btype='band')\n",
    "    filtered_ecg = signal.filtfilt(b, a, ecg_signal)\n",
    "    normalized_ecg = (filtered_ecg - np.mean(filtered_ecg)) / np.std(filtered_ecg)\n",
    "    return normalized_ecg\n",
    "\n",
    "fs = 700 # Sampling frequency (Hz)\n",
    "ecg_signal = stress_arrayy # Load your ECG data\n",
    "stress_array = preprocess_ecg(ecg_signal, fs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "fs = 700 # Sampling frequency (Hz)\n",
    "ecg_signal = stress_arrayy # Load your ECG data\n",
    "stress_array = preprocess_ecg(ecg_signal, fs)\n",
    "\n",
    "time = np.arange(0, len(ecg_signal)) / fs\n",
    "\n",
    "plt.figure(figsize=(12,10)) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the zoom range in seconds\n",
    "zoom_start = 0\n",
    "zoom_end = 10  # Zoom into the first 10 seconds\n",
    "\n",
    "# Plot original ECG signal\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(time, ecg_signal, label='Original ECG Signal')\n",
    "plt.xlim(zoom_start, zoom_end)\n",
    "plt.title('Original Stress ECG Signal (Zoomed In)')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend()\n",
    "\n",
    "# Plot normalized ECG signal\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(time,stress_array, label='Normalized ECG Signal', color='orange')\n",
    "plt.xlim(zoom_start, zoom_end)\n",
    "plt.title('Normalized  Stress ECG Signal')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude (normalized)')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import numpy as np\n",
    "import scipy.signal as signal\n",
    "def preprocess_ecg(ecg_signal, fs):\n",
    "    # Bandpass filter to remove noise\n",
    "    lowcut = 0.5  # 0.5 Hz\n",
    "    highcut = 45.0  # 45 Hz\n",
    "    nyquist = 0.5 * fs\n",
    "    low = lowcut / nyquist\n",
    "    high = highcut / nyquist\n",
    "    b, a = signal.butter(1, [low, high], btype='band')\n",
    "    filtered_ecg = signal.filtfilt(b, a, ecg_signal)\n",
    "    normalized_ecg = (filtered_ecg - np.mean(filtered_ecg)) / np.std(filtered_ecg)\n",
    "    return normalized_ecg\n",
    "\n",
    "fs = 700 # Sampling frequency (Hz)\n",
    "ecg_signal = amuse_arrayy # Load your ECG data\n",
    "amuse_array = preprocess_ecg(ecg_signal, fs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "fs = 700 # Sampling frequency (Hz)\n",
    "ecg_signal = amuse_arrayy\n",
    "amuse_array = preprocess_ecg(ecg_signal, fs)\n",
    "\n",
    "time = np.arange(0, len(ecg_signal)) / fs\n",
    "plt.figure(figsize=(12,10)) \n",
    "\n",
    "\n",
    "zoom_start = 0\n",
    "zoom_end = 10  \n",
    "\n",
    "# Plot original ECG signal\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(time, ecg_signal, label='Original ECG Signal')\n",
    "plt.xlim(zoom_start, zoom_end)\n",
    "plt.title('Original  Amusement ECG Signal (Zoomed In)')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend()\n",
    "\n",
    "# Plot normalized ECG signal\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(time,amuse_array, label='Normalized ECG Signal', color='orange')\n",
    "plt.xlim(zoom_start, zoom_end)\n",
    "plt.title('Normalized Amusement ECG Signal')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude (normalized)')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " segmentation and feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "pip install numpy scipy biosppy matplotlib\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from scipy.signal import welch\n",
    "\n",
    "# Example ECG signal (replace this with your actual signal)\n",
    " # Example data, replace with actual ECG signal\n",
    "\n",
    "# Input for window size and sampling rate\n",
    "window_size_seconds = int(input(\"Enter a window or segment size in seconds: \")) \n",
    "sampling_rate = int(input(\"Enter sampling frequency in Hz: \"))\n",
    "\n",
    "# Convert window size to number of samples\n",
    "window_size_samples = window_size_seconds * sampling_rate\n",
    "\n",
    "# Segment the signal into non-overlapping windows\n",
    "segments = []\n",
    "for start in range(0, len(baseline_array), window_size_samples):\n",
    "    end = start + window_size_samples\n",
    "    if end <= len(baseline_array):\n",
    "        segment = baseline_array[start:end]\n",
    "        segments.append(segment)\n",
    "\n",
    "def extract_features(segment, fs):\n",
    "    \"\"\"\n",
    "    Extract features from a segment of time-series data.\n",
    "    \n",
    "    Parameters:\n",
    "        segment (numpy array): The input time-series segment.\n",
    "        fs (int): Sampling frequency of the data in Hz.\n",
    "\n",
    "    Returns:\n",
    "        list: A list containing the extracted features.\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    \n",
    "    # Time-domain features\n",
    "    features.append(np.mean(segment))\n",
    "    features.append(np.std(segment))\n",
    "    features.append(np.min(segment))\n",
    "    features.append(np.max(segment))\n",
    "    \n",
    "    nn_intervals = np.diff(segment)\n",
    "    nn50 = sum(np.abs(nn_intervals) > 50)\n",
    "    rmssd = np.sqrt(np.mean(nn_intervals ** 2))\n",
    "    features.append(nn50)\n",
    "    features.append(rmssd)\n",
    "    \n",
    "    triangular_index = len(segment) / np.max(np.histogram(segment, bins='auto')[0])\n",
    "    features.append(triangular_index)\n",
    "    \n",
    "    features.append(np.sqrt(np.mean(segment**2)))  # RMS\n",
    "    features.append(stats.skew(segment))\n",
    "    features.append(stats.kurtosis(segment))\n",
    "    \n",
    "    # Frequency-domain features using Welch’s method\n",
    "    f, Pxx = welch(segment, fs=fs, nperseg=len(segment)//8)\n",
    "    power_low = np.trapz(Pxx[(f >= 0.5) & (f < 4)])\n",
    "    power_mid = np.trapz(Pxx[(f >= 4) & (f < 8)])\n",
    "    power_high = np.trapz(Pxx[(f >= 8) & (f < 13)])\n",
    "    \n",
    "    features.append(power_low)\n",
    "    features.append(power_mid)\n",
    "    features.append(power_high)\n",
    "    \n",
    "    return features\n",
    "\n",
    "feature_names = [\n",
    "    \"Mean\", \"Standard Deviation\", \"Minimum\", \"Maximum\", \"NN50\",\n",
    "    \"RMSSD\", \"Triangular Index\", \"RMS\",\n",
    "    \"Skewness\", \"Kurtosis\", \"power_low\", \n",
    "    \"power_mid\", \"power_high\"\n",
    "]\n",
    "\n",
    "# Extract features from each segment\n",
    "extracted_features = []\n",
    "for segment in segments:\n",
    "    features = extract_features(segment, sampling_rate)\n",
    "    extracted_features.append(features)\n",
    "\n",
    "# Convert the list of features to a NumPy array\n",
    "baseline_extracted_features = np.array(extracted_features)\n",
    "\n",
    "# Print the extracted features\n",
    "print(\"Feature Names:\")\n",
    "print(feature_names)\n",
    "print(\"\\nExtracted Features Shape:\", baseline_extracted_features.shape)\n",
    "print(\"Extracted Features for baseline array:\")\n",
    "print(baseline_extracted_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from scipy.signal import welch\n",
    "\n",
    "# Example ECG signal (replace this with your actual signal)\n",
    " # Example data, replace with actual ECG signal\n",
    "\n",
    "# Input for window size and sampling rate\n",
    "window_size_seconds = int(input(\"Enter a window or segment size in seconds: \")) \n",
    "sampling_rate = int(input(\"Enter sampling frequency in Hz: \"))\n",
    "\n",
    "# Convert window size to number of samples\n",
    "window_size_samples = window_size_seconds * sampling_rate\n",
    "\n",
    "# Segment the signal into non-overlapping windows\n",
    "segments = []\n",
    "for start in range(0, len(stress_array), window_size_samples):\n",
    "    end = start + window_size_samples\n",
    "    if end <= len(stress_array):\n",
    "        segment = stress_array[start:end]\n",
    "        segments.append(segment)\n",
    "\n",
    "def extract_features(segment, fs):\n",
    "    \"\"\"\n",
    "    Extract features from a segment of time-series data.\n",
    "    \n",
    "    Parameters:\n",
    "        segment (numpy array): The input time-series segment.\n",
    "        fs (int): Sampling frequency of the data in Hz.\n",
    "\n",
    "    Returns:\n",
    "        list: A list containing the extracted features.\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    \n",
    "    # Time-domain features\n",
    "    features.append(np.mean(segment))\n",
    "    features.append(np.std(segment))\n",
    "    features.append(np.min(segment))\n",
    "    features.append(np.max(segment))\n",
    "    \n",
    "    nn_intervals = np.diff(segment)\n",
    "    nn50 = sum(np.abs(nn_intervals) > 50)\n",
    "    rmssd = np.sqrt(np.mean(nn_intervals ** 2))\n",
    "    features.append(nn50)\n",
    "    features.append(rmssd)\n",
    "    \n",
    "    triangular_index = len(segment) / np.max(np.histogram(segment, bins='auto')[0])\n",
    "    features.append(triangular_index)\n",
    "    \n",
    "    features.append(np.sqrt(np.mean(segment**2)))  # RMS\n",
    "    features.append(stats.skew(segment))\n",
    "    features.append(stats.kurtosis(segment))\n",
    "    \n",
    "    # Frequency-domain features using Welch’s method\n",
    "    f, Pxx = welch(segment, fs=fs, nperseg=len(segment)//8)\n",
    "    power_low = np.trapz(Pxx[(f >= 0.5) & (f < 4)])\n",
    "    power_mid = np.trapz(Pxx[(f >= 4) & (f < 8)])\n",
    "    power_high = np.trapz(Pxx[(f >= 8) & (f < 13)])\n",
    "    \n",
    "    features.append(power_low)\n",
    "    features.append(power_mid)\n",
    "    features.append(power_high)\n",
    "    \n",
    "    return features\n",
    "\n",
    "feature_names = [\n",
    "    \"Mean\", \"Standard Deviation\", \"Minimum\", \"Maximum\", \"NN50\",\n",
    "    \"RMSSD\", \"Triangular Index\", \"RMS\",\n",
    "    \"Skewness\", \"Kurtosis\", \"power_low\", \n",
    "    \"power_mid\", \"power_high\"\n",
    "]\n",
    "\n",
    "# Extract features from each segment\n",
    "extracted_features = []\n",
    "for segment in segments:\n",
    "    features = extract_features(segment, sampling_rate)\n",
    "    extracted_features.append(features)\n",
    "\n",
    "# Convert the list of features to a NumPy array\n",
    "stress_extracted_features = np.array(extracted_features)\n",
    "\n",
    "# Print the extracted features\n",
    "print(\"Feature Names:\")\n",
    "print(feature_names)\n",
    "print(\"\\nExtracted Features Shape:\", stress_extracted_features.shape)\n",
    "print(\"Extracted Features for stress array:\")\n",
    "print(stress_extracted_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from scipy.signal import welch\n",
    "\n",
    "# Example ECG signal (replace this with your actual signal)\n",
    " # Example data, replace with actual ECG signal\n",
    "\n",
    "# Input for window size and sampling rate\n",
    "window_size_seconds = int(input(\"Enter a window or segment size in seconds: \")) \n",
    "sampling_rate = int(input(\"Enter sampling frequency in Hz: \"))\n",
    "\n",
    "# Convert w10\n",
    "# indow size to number of samples\n",
    "window_size_samples = window_size_seconds * sampling_rate\n",
    "\n",
    "# Segment the signal into non-overlapping windows\n",
    "segments = []\n",
    "for start in range(0, len(amuse_array), window_size_samples):\n",
    "    end = start + window_size_samples\n",
    "    if end <= len(amuse_array):\n",
    "        segment = amuse_array[start:end]\n",
    "        segments.append(segment)\n",
    "\n",
    "def extract_features(segment, fs):\n",
    "    \"\"\"\n",
    "    Extract features from a segment of time-series data.\n",
    "    \n",
    "    Parameters:\n",
    "        segment (numpy array): The input time-series segment.\n",
    "        fs (int): Sampling frequency of the data in Hz.\n",
    "\n",
    "    Returns:\n",
    "        list: A list containing the extracted features.\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    \n",
    "    # Time-domain features\n",
    "    features.append(np.mean(segment))\n",
    "    features.append(np.std(segment))\n",
    "    features.append(np.min(segment))\n",
    "    features.append(np.max(segment))\n",
    "    \n",
    "    nn_intervals = np.diff(segment)\n",
    "    nn50 = sum(np.abs(nn_intervals) > 50)\n",
    "    rmssd = np.sqrt(np.mean(nn_intervals ** 2))\n",
    "    features.append(nn50)\n",
    "    features.append(rmssd)\n",
    "    \n",
    "    triangular_index = len(segment) / np.max(np.histogram(segment, bins='auto')[0])\n",
    "    features.append(triangular_index)\n",
    "    \n",
    "    features.append(np.sqrt(np.mean(segment**2)))  # RMS\n",
    "    features.append(stats.skew(segment))\n",
    "    features.append(stats.kurtosis(segment))\n",
    "    \n",
    "    # Frequency-domain features using Welch’s method\n",
    "    f, Pxx = welch(segment, fs=fs, nperseg=len(segment)//8)\n",
    "    power_low = np.trapz(Pxx[(f >= 0.5) & (f < 4)])\n",
    "    power_mid = np.trapz(Pxx[(f >= 4) & (f < 8)])\n",
    "    power_high = np.trapz(Pxx[(f >= 8) & (f < 13)])\n",
    "    \n",
    "    features.append(power_low)\n",
    "    features.append(power_mid)\n",
    "    features.append(power_high)\n",
    "    \n",
    "    return features\n",
    "\n",
    "feature_names = [\n",
    "    \"Mean\", \"Standard Deviation\", \"Minimum\", \"Maximum\", \"NN50\",\n",
    "    \"RMSSD\", \"Triangular Index\", \"RMS\",\n",
    "    \"Skewness\", \"Kurtosis\", \"power_low\", \n",
    "    \"power_mid\", \"power_high\"\n",
    "]\n",
    "\n",
    "# Extract features from each segment\n",
    "extracted_features = []\n",
    "for segment in segments:\n",
    "    features = extract_features(segment, sampling_rate)\n",
    "    extracted_features.append(features)\n",
    "\n",
    "# Convert the list of features to a NumPy array\n",
    "amuse_extracted_features = np.array(extracted_features)\n",
    "\n",
    "# Print the extracted features\n",
    "print(\"Feature Names:\")\n",
    "print(feature_names)\n",
    "print(\"\\nExtracted Features Shape:\", amuse_extracted_features.shape)\n",
    "print(\"Extracted Features for amuse array:\")\n",
    "print(amuse_extracted_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import numpy as np\n",
    "\n",
    "# Create three example NumPy arrays\n",
    "array1 = baseline_extracted_features\n",
    "array2 = stress_extracted_features\n",
    "array3 = amuse_extracted_features\n",
    "# Combine the arrays into one\n",
    "features_array = np.concatenate((array1, array2, array3))\n",
    "\n",
    "print(features_array)\n",
    "print(len(features_array))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "pip install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Number of segments for each label\n",
    "label_1_count = len(baseline_extracted_features)\n",
    "label_2_count = len(stress_extracted_features)\n",
    "label_3_count = len(amuse_extracted_features)\n",
    "\n",
    "\n",
    "# Creating arrays for each label\n",
    "labels_1 = np.ones(label_1_count)\n",
    "labels_2 = np.full(label_2_count, 2)\n",
    "labels_3 = np.full(label_3_count, 3)\n",
    "\n",
    "# Combine all labels into one array\n",
    "all_labels = np.concatenate([labels_1, labels_2, labels_3])\n",
    "print((len(all_labels)))\n",
    "\n",
    "# Optionally shuffle the labels if needed\n",
    "\n",
    "\n",
    "# Create a DataFrame\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Load your feature data and labels\n",
    "# Replace with your actual features and labels\n",
    "features = features_array\n",
    "labels = all_labels\n",
    "\n",
    "# Print the lengths of features and labels\n",
    "print('Number of samples in features:', len(features))\n",
    "print('Number of samples in labels:', len(labels))\n",
    "\n",
    "# Split the data\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train and evaluate SVM model\n",
    "svm_model = SVC(kernel='poly')\n",
    "svm_model.fit(X_train, y_train)\n",
    "y_val_pred_svm = svm_model.predict(X_val)\n",
    "print('SVM Validation Accuracy:', accuracy_score(y_val, y_val_pred_svm))\n",
    "print('SVM Validation Report:')\n",
    "print(classification_report(y_val, y_val_pred_svm))\n",
    "\n",
    "# Train and evaluate k-NN model\n",
    "knn_model = KNeighborsClassifier(n_neighbors=3)\n",
    "knn_model.fit(X_train, y_train)\n",
    "y_val_pred_knn = knn_model.predict(X_val)\n",
    "print('k-NN Validation Accuracy:', accuracy_score(y_val, y_val_pred_knn))\n",
    "print('k-NN Validation Report:')\n",
    "print(classification_report(y_val, y_val_pred_knn))\n",
    "\n",
    "# Train and evaluate Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_val_pred_rf = rf_model.predict(X_val)\n",
    "print('Random Forest Validation Accuracy:', accuracy_score(y_val, y_val_pred_rf))\n",
    "print('Random Forest Validation Report:')\n",
    "print(classification_report(y_val, y_val_pred_rf))\n",
    "\n",
    "# Train and evaluate Logistic Regression model\n",
    "log_reg_model = LogisticRegression(max_iter=500)\n",
    "log_reg_model.fit(X_train, y_train)\n",
    "y_val_pred_log_reg = log_reg_model.predict(X_val)\n",
    "print('Logistic Regression Validation Accuracy:', accuracy_score(y_val, y_val_pred_log_reg))\n",
    "print('Logistic Regression Validation Report:')\n",
    "print(classification_report(y_val, y_val_pred_log_reg))\n",
    "\n",
    "# Train and evaluate MLP model\n",
    "mlp_model = MLPClassifier(hidden_layer_sizes=(100, ), max_iter=500)\n",
    "mlp_model.fit(X_train, y_train)\n",
    "y_val_pred_mlp = mlp_model.predict(X_val)\n",
    "print('MLP Validation Accuracy:', accuracy_score(y_val, y_val_pred_mlp))\n",
    "print('MLP Validation Report:')\n",
    "print(classification_report(y_val, y_val_pred_mlp))\n",
    "\n",
    "# Evaluate the models on the test set\n",
    "# Choose the best model based on validation performance\n",
    "y_test_pred_rf =  mlp_model.predict(X_test)\n",
    "print('MLP Test Accuracy:', accuracy_score(y_test, y_test_pred_rf))\n",
    "print('MLP Forest Test Report:')\n",
    "print(classification_report(y_test, y_test_pred_rf))\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
